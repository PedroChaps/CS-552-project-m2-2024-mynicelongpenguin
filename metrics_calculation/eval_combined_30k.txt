Install Dependency completed
Going to run the program...
ARGS
Namespace(model_path_or_name='/scratch/izar/hogenhau/project-m2-2024-mynicelongpenguin/model/models/outputs/combined_30k_model', dataset_path='/scratch/izar/hogenhau/project-m2-2024-mynicelongpenguin/data/combined_test.jsonl', batch_size=4, base_model_name='rhysjones/phi-2-orange-v2', sample_quantity=5000)


Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:14<00:14, 14.42s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  6.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  8.00s/it]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
MODEL IS OF PHIFORCAUSALLM
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 5649 examples [00:00, 42755.00 examples/s]Generating train split: 15955 examples [00:00, 70522.30 examples/s]Generating train split: 15955 examples [00:00, 65779.57 examples/s]
Length of vanilla dataset: 5000
Map (num_proc=8):   0%|          | 0/5000 [00:00<?, ? examples/s]Map (num_proc=8):  12%|█▎        | 625/5000 [00:00<00:06, 712.63 examples/s]Map (num_proc=8):  50%|█████     | 2500/5000 [00:01<00:00, 2979.31 examples/s]Map (num_proc=8):  75%|███████▌  | 3750/5000 [00:01<00:00, 3982.02 examples/s]Map (num_proc=8): 100%|██████████| 5000/5000 [00:01<00:00, 3685.98 examples/s]
Length of processed dataset: 5000
Eval:   0%|          | 0/1250 [00:00<?, ?it/s]Eval:   1%|          | 10/1250 [00:06<12:30,  1.65it/s]Eval:   2%|▏         | 20/1250 [00:50<58:47,  2.87s/it]Eval:   2%|▏         | 30/1250 [01:35<1:13:37,  3.62s/it]Eval:   3%|▎         | 40/1250 [02:21<1:20:24,  3.99s/it]Eval:   4%|▍         | 50/1250 [03:06<1:23:52,  4.19s/it]Eval:   5%|▍         | 60/1250 [03:52<1:25:42,  4.32s/it]Eval:   6%|▌         | 70/1250 [04:38<1:26:35,  4.40s/it]Eval:   6%|▋         | 80/1250 [05:23<1:26:55,  4.46s/it]Eval:   7%|▋         | 90/1250 [06:09<1:26:53,  4.49s/it]Eval:   8%|▊         | 100/1250 [06:55<1:26:38,  4.52s/it]Eval:   9%|▉         | 110/1250 [07:41<1:26:15,  4.54s/it]Eval:  10%|▉         | 120/1250 [08:27<1:25:43,  4.55s/it]Eval:  10%|█         | 130/1250 [09:12<1:25:07,  4.56s/it]Eval:  11%|█         | 140/1250 [09:58<1:24:28,  4.57s/it]Eval:  12%|█▏        | 150/1250 [10:44<1:23:46,  4.57s/it]Eval:  13%|█▎        | 160/1250 [11:30<1:23:03,  4.57s/it]Eval:  14%|█▎        | 170/1250 [12:16<1:22:20,  4.57s/it]Eval:  14%|█▍        | 180/1250 [13:01<1:21:36,  4.58s/it]Eval:  15%|█▌        | 190/1250 [13:47<1:20:52,  4.58s/it]Eval:  16%|█▌        | 200/1250 [14:33<1:20:07,  4.58s/it]Eval:  17%|█▋        | 210/1250 [15:19<1:19:22,  4.58s/it]Eval:  18%|█▊        | 220/1250 [16:05<1:18:36,  4.58s/it]Eval:  18%|█▊        | 230/1250 [16:50<1:17:50,  4.58s/it]Eval:  19%|█▉        | 240/1250 [17:36<1:17:04,  4.58s/it]Eval:  20%|██        | 250/1250 [18:22<1:16:19,  4.58s/it]Eval:  21%|██        | 260/1250 [19:08<1:15:33,  4.58s/it]Eval:  22%|██▏       | 270/1250 [19:54<1:14:47,  4.58s/it]Eval:  22%|██▏       | 280/1250 [20:39<1:14:01,  4.58s/it]Eval:  23%|██▎       | 290/1250 [21:25<1:13:16,  4.58s/it]Eval:  24%|██▍       | 300/1250 [22:11<1:12:30,  4.58s/it]Eval:  25%|██▍       | 310/1250 [22:57<1:11:44,  4.58s/it]Eval:  26%|██▌       | 320/1250 [23:43<1:10:59,  4.58s/it]Eval:  26%|██▋       | 330/1250 [24:28<1:10:13,  4.58s/it]Eval:  27%|██▋       | 340/1250 [25:14<1:09:29,  4.58s/it]Eval:  28%|██▊       | 350/1250 [26:00<1:08:42,  4.58s/it]Eval:  29%|██▉       | 360/1250 [26:46<1:07:57,  4.58s/it]Eval:  30%|██▉       | 370/1250 [27:32<1:07:10,  4.58s/it]Eval:  30%|███       | 380/1250 [28:17<1:06:25,  4.58s/it]Eval:  31%|███       | 390/1250 [29:03<1:05:40,  4.58s/it]Eval:  32%|███▏      | 400/1250 [29:49<1:04:54,  4.58s/it]Eval:  33%|███▎      | 410/1250 [30:35<1:04:09,  4.58s/it]Eval:  34%|███▎      | 420/1250 [31:21<1:03:23,  4.58s/it]Eval:  34%|███▍      | 430/1250 [32:07<1:02:38,  4.58s/it]Eval:  35%|███▌      | 440/1250 [32:52<1:01:52,  4.58s/it]Eval:  36%|███▌      | 450/1250 [33:38<1:01:05,  4.58s/it]Eval:  37%|███▋      | 460/1250 [34:24<1:00:19,  4.58s/it]Eval:  38%|███▊      | 470/1250 [35:10<59:33,  4.58s/it]  Eval:  38%|███▊      | 480/1250 [35:56<58:46,  4.58s/it]Eval:  39%|███▉      | 490/1250 [36:41<58:01,  4.58s/it]Eval:  40%|████      | 500/1250 [37:27<57:14,  4.58s/it]Eval:  41%|████      | 510/1250 [38:13<56:28,  4.58s/it]Eval:  42%|████▏     | 520/1250 [38:59<55:43,  4.58s/it]Eval:  42%|████▏     | 530/1250 [39:45<54:57,  4.58s/it]Eval:  43%|████▎     | 540/1250 [40:30<54:11,  4.58s/it]Eval:  44%|████▍     | 550/1250 [41:16<53:26,  4.58s/it]Eval:  45%|████▍     | 560/1250 [42:02<52:39,  4.58s/it]Eval:  46%|████▌     | 570/1250 [42:48<51:54,  4.58s/it]Eval:  46%|████▋     | 580/1250 [43:34<51:08,  4.58s/it]Eval:  47%|████▋     | 590/1250 [44:19<50:22,  4.58s/it]Eval:  48%|████▊     | 600/1250 [45:05<49:37,  4.58s/it]Eval:  49%|████▉     | 610/1250 [45:51<48:52,  4.58s/it]Eval:  50%|████▉     | 620/1250 [46:37<48:06,  4.58s/it]Eval:  50%|█████     | 630/1250 [47:23<47:20,  4.58s/it]Eval:  51%|█████     | 640/1250 [48:08<46:34,  4.58s/it]Eval:  52%|█████▏    | 650/1250 [48:54<45:48,  4.58s/it]Eval:  53%|█████▎    | 660/1250 [49:40<45:02,  4.58s/it]Eval:  54%|█████▎    | 670/1250 [50:26<44:16,  4.58s/it]Eval:  54%|█████▍    | 680/1250 [51:12<43:30,  4.58s/it]Eval:  55%|█████▌    | 690/1250 [51:57<42:44,  4.58s/it]Eval:  56%|█████▌    | 700/1250 [52:43<41:58,  4.58s/it]Eval:  57%|█████▋    | 710/1250 [53:29<41:13,  4.58s/it]Eval:  58%|█████▊    | 720/1250 [54:15<40:27,  4.58s/it]Eval:  58%|█████▊    | 730/1250 [55:01<39:42,  4.58s/it]Eval:  59%|█████▉    | 740/1250 [55:47<38:57,  4.58s/it]Eval:  60%|██████    | 750/1250 [56:32<38:10,  4.58s/it]Eval:  61%|██████    | 760/1250 [57:18<37:24,  4.58s/it]Eval:  62%|██████▏   | 770/1250 [58:04<36:39,  4.58s/it]Eval:  62%|██████▏   | 780/1250 [58:50<35:53,  4.58s/it]Eval:  63%|██████▎   | 790/1250 [59:36<35:07,  4.58s/it]Eval:  64%|██████▍   | 800/1250 [1:00:21<34:21,  4.58s/it]Eval:  65%|██████▍   | 810/1250 [1:01:07<33:36,  4.58s/it]Eval:  66%|██████▌   | 820/1250 [1:01:53<32:50,  4.58s/it]Eval:  66%|██████▋   | 830/1250 [1:02:39<32:04,  4.58s/it]Eval:  67%|██████▋   | 840/1250 [1:03:25<31:19,  4.58s/it]Eval:  68%|██████▊   | 850/1250 [1:04:11<30:33,  4.58s/it]Eval:  69%|██████▉   | 860/1250 [1:04:57<29:47,  4.58s/it]Eval:  70%|██████▉   | 870/1250 [1:05:42<29:02,  4.59s/it]Eval:  70%|███████   | 880/1250 [1:06:28<28:16,  4.58s/it]Eval:  71%|███████   | 890/1250 [1:07:14<27:30,  4.58s/it]Eval:  72%|███████▏  | 900/1250 [1:08:00<26:44,  4.58s/it]Eval:  73%|███████▎  | 910/1250 [1:08:46<25:59,  4.59s/it]Eval:  74%|███████▎  | 920/1250 [1:09:32<25:13,  4.59s/it]Eval:  74%|███████▍  | 930/1250 [1:10:18<24:27,  4.59s/it]Eval:  75%|███████▌  | 940/1250 [1:11:03<23:41,  4.59s/it]Eval:  76%|███████▌  | 950/1250 [1:11:49<22:55,  4.59s/it]Eval:  77%|███████▋  | 960/1250 [1:12:35<22:09,  4.59s/it]Eval:  78%|███████▊  | 970/1250 [1:13:21<21:23,  4.59s/it]Eval:  78%|███████▊  | 980/1250 [1:14:07<20:38,  4.59s/it]Eval:  79%|███████▉  | 990/1250 [1:14:53<19:52,  4.59s/it]Eval:  80%|████████  | 1000/1250 [1:15:39<19:06,  4.59s/it]Eval:  81%|████████  | 1010/1250 [1:16:24<18:20,  4.59s/it]Eval:  82%|████████▏ | 1020/1250 [1:17:10<17:35,  4.59s/it]Eval:  82%|████████▏ | 1030/1250 [1:17:56<16:49,  4.59s/it]Eval:  83%|████████▎ | 1040/1250 [1:18:42<16:03,  4.59s/it]Eval:  84%|████████▍ | 1050/1250 [1:19:28<15:17,  4.59s/it]Eval:  85%|████████▍ | 1060/1250 [1:20:14<14:32,  4.59s/it]Eval:  86%|████████▌ | 1070/1250 [1:21:00<13:46,  4.59s/it]Eval:  86%|████████▋ | 1080/1250 [1:21:46<13:00,  4.59s/it]Eval:  87%|████████▋ | 1090/1250 [1:22:32<12:14,  4.59s/it]Eval:  88%|████████▊ | 1100/1250 [1:23:18<11:28,  4.59s/it]Eval:  89%|████████▉ | 1110/1250 [1:24:03<10:42,  4.59s/it]Eval:  90%|████████▉ | 1120/1250 [1:24:49<09:56,  4.59s/it]Eval:  90%|█████████ | 1130/1250 [1:25:35<09:10,  4.59s/it]Eval:  91%|█████████ | 1140/1250 [1:26:21<08:24,  4.59s/it]Eval:  92%|█████████▏| 1150/1250 [1:27:07<07:38,  4.59s/it]Eval:  93%|█████████▎| 1160/1250 [1:27:53<06:52,  4.59s/it]Eval:  94%|█████████▎| 1170/1250 [1:28:39<06:06,  4.59s/it]Eval:  94%|█████████▍| 1180/1250 [1:29:24<05:21,  4.59s/it]Eval:  95%|█████████▌| 1190/1250 [1:30:10<04:35,  4.59s/it]Eval:  96%|█████████▌| 1200/1250 [1:30:56<03:49,  4.59s/it]Eval:  97%|█████████▋| 1210/1250 [1:31:42<03:03,  4.59s/it]Eval:  98%|█████████▊| 1220/1250 [1:32:28<02:17,  4.59s/it]Eval:  98%|█████████▊| 1230/1250 [1:33:14<01:31,  4.59s/it]Eval:  99%|█████████▉| 1240/1250 [1:34:00<00:45,  4.59s/it]Eval: 100%|██████████| 1250/1250 [1:34:45<00:00,  4.59s/it]Eval: 100%|██████████| 1250/1250 [1:35:27<00:00,  4.58s/it]
Batch 0 loss: 1.8960891962051392
Batch 10 loss: 1.8454976081848145
Batch 20 loss: 3.28206205368042
Batch 30 loss: 2.8496789932250977
Batch 40 loss: 3.1867194175720215
Batch 50 loss: 2.14237642288208
Batch 60 loss: 2.495783567428589
Batch 70 loss: 2.4021952152252197
Batch 80 loss: 2.6180496215820312
Batch 90 loss: 2.990433692932129
Batch 100 loss: 1.2889502048492432
Batch 110 loss: 2.8539164066314697
Batch 120 loss: 2.566844940185547
Batch 130 loss: 2.6281943321228027
Batch 140 loss: 3.4969255924224854
Batch 150 loss: 2.4138267040252686
Batch 160 loss: 1.7626692056655884
Batch 170 loss: 1.5589919090270996
Batch 180 loss: 2.7441840171813965
Batch 190 loss: 2.5794761180877686
Batch 200 loss: 2.257145404815674
Batch 210 loss: 3.9178621768951416
Batch 220 loss: 2.041982650756836
Batch 230 loss: 2.2317488193511963
Batch 240 loss: 1.8729876279830933
Batch 250 loss: 1.857730746269226
Batch 260 loss: 1.5858057737350464
Batch 270 loss: 3.238623857498169
Batch 280 loss: 3.895217180252075
Batch 290 loss: 3.3350746631622314
Batch 300 loss: 2.5434603691101074
Batch 310 loss: 1.4307235479354858
Batch 320 loss: 2.640810251235962
Batch 330 loss: 1.3497929573059082
Batch 340 loss: 2.4756791591644287
Batch 350 loss: 2.547227621078491
Batch 360 loss: 2.8574726581573486
Batch 370 loss: 2.504936933517456
Batch 380 loss: 2.5354669094085693
Batch 390 loss: 1.2578837871551514
Batch 400 loss: 1.2489864826202393
Batch 410 loss: 1.8378396034240723
Batch 420 loss: 2.402726173400879
Batch 430 loss: 1.9285396337509155
Batch 440 loss: 2.0744450092315674
Batch 450 loss: 1.6746293306350708
Batch 460 loss: 3.67927622795105
Batch 470 loss: 1.9227441549301147
Batch 480 loss: 1.5969282388687134
Batch 490 loss: 3.717439889907837
Batch 500 loss: 1.7277358770370483
Batch 510 loss: 1.9735981225967407
Batch 520 loss: 2.3263118267059326
Batch 530 loss: 1.7343658208847046
Batch 540 loss: 2.2776060104370117
Batch 550 loss: 3.7809503078460693
Batch 560 loss: 2.7300796508789062
Batch 570 loss: 2.270498037338257
Batch 580 loss: 2.219397783279419
Batch 590 loss: 2.371969223022461
Batch 600 loss: 1.5843178033828735
Batch 610 loss: 2.5547568798065186
Batch 620 loss: 2.1323728561401367
Batch 630 loss: 2.220860242843628
Batch 640 loss: 2.1723520755767822
Batch 650 loss: 3.562110185623169
Batch 660 loss: 3.3881068229675293
Batch 670 loss: 2.600635528564453
Batch 680 loss: 3.0401558876037598
Batch 690 loss: 2.080986738204956
Batch 700 loss: 3.062840223312378
Batch 710 loss: 2.1534061431884766
Batch 720 loss: 1.7226357460021973
Batch 730 loss: 1.2321487665176392
Batch 740 loss: 2.8824946880340576
Batch 750 loss: 2.0766773223876953
Batch 760 loss: 2.32142972946167
Batch 770 loss: 2.24090838432312
Batch 780 loss: 1.8633689880371094
Batch 790 loss: 2.517592430114746
Batch 800 loss: 2.356227159500122
Batch 810 loss: 2.2133595943450928
Batch 820 loss: 2.3035495281219482
Batch 830 loss: 2.2245452404022217
Batch 840 loss: 2.1584222316741943
Batch 850 loss: 2.437549114227295
Batch 860 loss: 2.317742109298706
Batch 870 loss: 2.3372933864593506
Batch 880 loss: 2.5249404907226562
Batch 890 loss: 2.867642879486084
Batch 900 loss: 1.6457552909851074
Batch 910 loss: 2.2539267539978027
Batch 920 loss: 2.608128547668457
Batch 930 loss: 3.2230663299560547
Batch 940 loss: 1.9936307668685913
Batch 950 loss: 2.6022472381591797
Batch 960 loss: 2.471102476119995
Batch 970 loss: 2.0236010551452637
Batch 980 loss: 2.1162126064300537
Batch 990 loss: 2.6537458896636963
Batch 1000 loss: 2.5960779190063477
Batch 1010 loss: 2.7946081161499023
Batch 1020 loss: 2.8307180404663086
Batch 1030 loss: 1.7711374759674072
Batch 1040 loss: 3.5202322006225586
Batch 1050 loss: 1.5352277755737305
Batch 1060 loss: 2.3645405769348145
Batch 1070 loss: 2.9350762367248535
Batch 1080 loss: 2.0263617038726807
Batch 1090 loss: 2.829798936843872
Batch 1100 loss: 3.0371227264404297
Batch 1110 loss: 2.926119327545166
Batch 1120 loss: 1.6113377809524536
Batch 1130 loss: 3.366562604904175
Batch 1140 loss: 3.889017105102539
Batch 1150 loss: 2.548844337463379
Batch 1160 loss: 2.272920846939087
Batch 1170 loss: 1.1402137279510498
Batch 1180 loss: 1.3470834493637085
Batch 1190 loss: 1.6523793935775757
Batch 1200 loss: 1.7882109880447388
Batch 1210 loss: 1.8163871765136719
Batch 1220 loss: 2.1621062755584717
Batch 1230 loss: 2.3434700965881348
Batch 1240 loss: 2.0749905109405518
Average Loss: 2.431325194454193
Test complete on i16
